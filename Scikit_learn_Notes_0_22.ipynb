{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scikit learn Notes 0.22",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manasvee3LOQ/Deep-math-machine-learning.ai/blob/master/Scikit_learn_Notes_0_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Teq3-utx_rOe",
        "colab_type": "code",
        "outputId": "5cc0574f-3961-450b-8cf1-c8fbefab9ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import neighbors, datasets, preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data[:, :2], iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33)\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.631578947368421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za8aUk3QAwEX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "preprocessing.Binarizer([threshold, copy])\n",
        "\n",
        "Binarize data (set feature values to 0 or 1) according to a threshold\n",
        "\n",
        "preprocessing.FunctionTransformer([func, …])\n",
        "\n",
        "Constructs a transformer from an arbitrary callable.\n",
        "\n",
        "preprocessing.KBinsDiscretizer([n_bins, …])\n",
        "\n",
        "Bin continuous data into intervals.\n",
        "\n",
        "preprocessing.KernelCenterer()\n",
        "\n",
        "Center a kernel matrix\n",
        "\n",
        "preprocessing.LabelBinarizer([neg_label, …])\n",
        "\n",
        "Binarize labels in a one-vs-all fashion\n",
        "\n",
        "preprocessing.LabelEncoder\n",
        "\n",
        "Encode target labels with value between 0 and n_classes-1.\n",
        "\n",
        "preprocessing.MultiLabelBinarizer([classes, …])\n",
        "\n",
        "Transform between iterable of iterables and a multilabel format\n",
        "\n",
        "preprocessing.MaxAbsScaler([copy])\n",
        "\n",
        "Scale each feature by its maximum absolute value.\n",
        "\n",
        "preprocessing.MinMaxScaler([feature_range, copy])\n",
        "\n",
        "Transform features by scaling each feature to a given range.\n",
        "\n",
        "preprocessing.Normalizer([norm, copy])\n",
        "\n",
        "Normalize samples individually to unit norm.\n",
        "\n",
        "preprocessing.OneHotEncoder([categories, …])\n",
        "\n",
        "Encode categorical features as a one-hot numeric array.\n",
        "\n",
        "preprocessing.OrdinalEncoder([categories, dtype])\n",
        "\n",
        "Encode categorical features as an integer array.\n",
        "\n",
        "preprocessing.PolynomialFeatures([degree, …])\n",
        "\n",
        "Generate polynomial and interaction features.\n",
        "\n",
        "preprocessing.PowerTransformer([method, …])\n",
        "\n",
        "Apply a power transform featurewise to make data more Gaussian-like.\n",
        "\n",
        "preprocessing.QuantileTransformer([…])\n",
        "\n",
        "Transform features using quantiles information.\n",
        "\n",
        "preprocessing.RobustScaler([with_centering, …])\n",
        "\n",
        "Scale features using statistics that are robust to outliers.\n",
        "\n",
        "preprocessing.StandardScaler([copy, …])\n",
        "\n",
        "Standardize features by removing the mean and scaling to unit variance\n",
        "\n",
        "preprocessing.add_dummy_feature(X[, value])\n",
        "\n",
        "Augment dataset with an additional dummy feature.\n",
        "\n",
        "preprocessing.binarize(X[, threshold, copy])\n",
        "\n",
        "Boolean thresholding of array-like or scipy.sparse matrix\n",
        "\n",
        "preprocessing.label_binarize(y, classes[, …])\n",
        "\n",
        "Binarize labels in a one-vs-all fashion\n",
        "\n",
        "preprocessing.maxabs_scale(X[, axis, copy])\n",
        "\n",
        "Scale each feature to the [-1, 1] range without breaking the sparsity.\n",
        "\n",
        "preprocessing.minmax_scale(X[, …])\n",
        "\n",
        "Transform features by scaling each feature to a given range.\n",
        "\n",
        "preprocessing.normalize(X[, norm, axis, …])\n",
        "\n",
        "Scale input vectors individually to unit norm (vector length).\n",
        "\n",
        "preprocessing.quantile_transform(X[, axis, …])\n",
        "\n",
        "Transform features using quantiles information.\n",
        "\n",
        "preprocessing.robust_scale(X[, axis, …])\n",
        "\n",
        "Standardize a dataset along any axis\n",
        "\n",
        "preprocessing.scale(X[, axis, with_mean, …])\n",
        "\n",
        "Standardize a dataset along any axis\n",
        "\n",
        "preprocessing.power_transform(X[, method, …])\n",
        "\n",
        "Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMGPLYbHANla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#Training And Test Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n",
        "\n",
        "#PREPROCESSING THE DATA\n",
        "\n",
        "#Standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "standardized_X = scaler.transform(X_train)\n",
        "standardized_X_test = scaler.transform(X_test)\n",
        "\n",
        "#Normalization\n",
        "from sklearn.preprocessing import Normalizer\n",
        "normalizer = Normalizer().fit(X_train)\n",
        "normalized_X = normalizer.transform(X_train)\n",
        "normalized_X_test = normalizer.transform(X_test)\n",
        "\n",
        "#Binarization\n",
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer = Binarizer(threshold=0.0).fit(X)\n",
        "binary_X = binarizer.transform(X)\n",
        "\n",
        "#Encoding Categorical Features\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "#Imputing Missing Values\n",
        "from sklearn.preprocessing import Imputer\n",
        "imputer = Imputer(missing_values=0, strategy='mean', axis=0)\n",
        "imputer.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOUva1cPB4AT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DEFINING MODEL\n",
        "\n",
        "#Supervised Learning Estimators\n",
        "\n",
        "#Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression(normalize=True)\n",
        "\n",
        "#Support Vector Machines (SVM)\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC(kernel='linear')\n",
        "\n",
        "#Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "\n",
        "#KNN\n",
        "from sklearn import neighbors\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "#Unsupervised Learning Estimators\n",
        "\n",
        "#Principal Component Analysis (PCA)\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.95)\n",
        "\n",
        "#K Means\n",
        "from sklearn.cluster import KMeans\n",
        "k_means = KMeans(n_clusters=3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM5cGRkXCdJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MODEL FITTING\n",
        "\n",
        "#Supervised learning\n",
        "\n",
        "lr.fit(X, y)\n",
        "knn.fit(X_train, y_train)\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "#Unsupervised Learning\n",
        "k_means.fit(X_train)\n",
        "pca_model = pca.fit_transform(X_train) #Fit to data, then transform it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2aEXLRXCzJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PREDICTION\n",
        "\n",
        "#Supervised Estimators\n",
        "y_pred = svc.predict ( np.random.random((2,5)) ) #Predict labels\n",
        "y_pred = lr.predict(X_test) #Predict labels\n",
        "y_pred = knn.predict_proba( X_test ) #Estimate probability of a labe\n",
        "\n",
        "#Unsupervised Estimators\n",
        "y_pred = k_means.predict(X_test) #Predict labels in clustering algos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF6dPQ2_DS_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate Your Model’s Performance\n",
        "\n",
        "#CLASSIFICATION METRICS\n",
        "\n",
        "#Accuracy Score\n",
        "knn.score(X_test, y_test) ##Estimator score method\n",
        "\n",
        "from sklearn.metrics import accuracy_score \n",
        "accuracy_score(y_test, y_pred) #Metric scoring functions\n",
        "\n",
        "#Classification Report\n",
        "from sklearn.metrics import classification_report #Precision, recall, f1-score and support\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "#REGRESSION METRICS\n",
        "\n",
        "#Mean Absolute Error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "y_true = [3, -0.5, 2]\n",
        "mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "#Mean Squared Error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y_test, y_pred)\n",
        "\n",
        "#R² Score\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_true, y_pred)\n",
        "\n",
        "#CLUSTERING METRICS\n",
        "\n",
        "#Adjusted Rand Index\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "adjusted_rand_score(y_true, y_pred)\n",
        "\n",
        "#Homogeneity\n",
        "from sklearn.metrics import homogeneity_score\n",
        "homogeneity_score(y_true, y_pred)\n",
        "\n",
        "#V-measure\n",
        "from sklearn.metrics import v_measure_score\n",
        "metrics.v_measure_score(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Dn4rHVE_vZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CROSS VALIDATION\n",
        "\n",
        "from sklearn.cross_validation import cross_val_score\n",
        "print(cross_val_score(knn, X_train, y_train, cv=4))\n",
        "print(cross_val_score(lr, X, y, cv=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csdKGBpxFIhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MODEL TUNING\n",
        "\n",
        "#GridSearch\n",
        "from sklearn.grid_search import GridSearchCV\n",
        "params = {\"n_neighbors\": np.arange(1,3),\n",
        "\"metric\": [\"euclidean\", \"cityblock\"]}\n",
        "grid = GridSearchCV(estimator=knn,\n",
        "param_grid=params)\n",
        "grid.fit(X_train, y_train)\n",
        "print(grid.best_score_)\n",
        "print(grid.best_estimator_.n_neighbors)\n",
        "\n",
        "#RANDOMIZED PARAMETER OPTIMIZATION\n",
        "\n",
        "from sklearn.grid_search import RandomizedSearchCV\n",
        "params = {\"n_neighbors\": range(1,5),\n",
        "\"weights\": [\"uniform\", \"distance\"]}\n",
        "rsearch = RandomizedSearchCV(estimator=knn,\n",
        "param_distributions=params,\n",
        "cv=4,\n",
        "n_iter=8,\n",
        "random_state=5)\n",
        "rsearch.fit(X_train, y_train)\n",
        "print(rsearch.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dydT_yyFcI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datasets.make_biclusters(shape, n_clusters)\n",
        "         #Generate an array with constant block diagonal structure for biclustering.\n",
        "datasets.make_blobs([n_samples, n_features, …])\n",
        "         #Generate isotropic Gaussian blobs for clustering.\n",
        "datasets.make_checkerboard(shape, n_clusters)\n",
        "         #Generate an array with block checkerboard structure for biclustering.\n",
        "datasets.make_circles([n_samples, shuffle, …])\n",
        "         #Make a large circle containing a smaller circle in 2d.\n",
        "datasets.make_classification([n_samples, …])\n",
        "         #Generate a random n-class classification problem.\n",
        "datasets.make_friedman1([n_samples, …])\n",
        "        #Generate the “Friedman #1” regression problem\n",
        "datasets.make_friedman2([n_samples, noise, …])\n",
        "         #Generate the “Friedman #2” regression problem\n",
        "datasets.make_friedman3([n_samples, noise, …])\n",
        "         #Generate the “Friedman #3” regression problem\n",
        "datasets.make_gaussian_quantiles([mean, …])\n",
        "         #Generate isotropic Gaussian and label samples by quantile\n",
        "datasets.make_hastie_10_2([n_samples, …])\n",
        "         #Generates data for binary classification used in Hastie et al.\n",
        "datasets.make_low_rank_matrix([n_samples, …])\n",
        "         #Generate a mostly low rank matrix with bell-shaped singular values\n",
        "datasets.make_moons([n_samples, shuffle, …])\n",
        "         #Make two interleaving half circles\n",
        "datasets.make_multilabel_classification([…])\n",
        "         #Generate a random multilabel classification problem.\n",
        "datasets.make_regression([n_samples, …])\n",
        "         #Generate a random regression problem.\n",
        "datasets.make_s_curve([n_samples, noise, …])\n",
        "         #Generate an S curve dataset.\n",
        "datasets.make_sparse_coded_signal(n_samples, …)\n",
        "         #Generate a signal as a sparse combination of dictionary elements.\n",
        "datasets.make_sparse_spd_matrix([dim, …])\n",
        "         #Generate a sparse symmetric definite positive matrix.\n",
        "datasets.make_sparse_uncorrelated([…])\n",
        "         #Generate a random regression problem with sparse uncorrelated design\n",
        "datasets.make_spd_matrix(n_dim[, random_state])\n",
        "         #Generate a random symmetric, positive-definite matrix.\n",
        "datasets.make_swiss_roll([n_samples, noise, …])\n",
        "         #Generate a swiss roll dataset."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}